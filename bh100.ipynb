{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00c67f2-d34d-457c-a3b4-1e7c7cd24914",
   "metadata": {},
   "source": [
    "# How to be Choosy: Billboard Hot 100\n",
    "_by Michelle Hoda Wilkerson_\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/CalCoRE/choosy/main?urlpath=%2Fdoc%2Ftree%2Fbh100.ipynb)  <- Click here to open an interactive version of this notebook!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> This notebook is written to illustrate the data wrangling moves described in [anonymized manuscript], using a dataset about songs that have charted on the Billboard Hot 100 charts (see <a href=\"#about\">here</a> for more information). Some of the text below is excerpted from the manuscript. Each section below corresponds to a subheading in the manuscript. See also How to be Choosy: 2022 CA Toxic Release Inventory. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c04d9-3172-4c1f-8f66-5943fde61862",
   "metadata": {},
   "source": [
    "This is a Jupyter Python notebook. It mixes together Python code and text into one runnable, editable document (like a Word file or Google Doc). When you see code, you can run it by clicking the \"Play\" button in your interface. You can also edit it and see what happens! Often, pieces of code that appear later in the document rely on piece of code that appear earlier. So if you are a beginner, it's useful to work through every chunk of the document in order, top to bottom, and try to understand what each part does before hopping around or making too many edits.\n",
    "\n",
    "With any Jupyter notebook, we begin by importing the necessary libraries. You should run the cell below before running any other code. Here, we are using the `pandas` library, an industry standard that provides us with special methods for reading in and working with data in Python. We also import the `datetime` library to help us process and sort date information in the dataset. Matplotlib helps us with data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49358233-5bb1-4f0a-a277-f06355ffa1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas # for data wrangling\n",
    "import datetime # to handle dates\n",
    "import seaborn as sns # for data visualization\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import numpy as np # for calculations\n",
    "\n",
    "# if there are more than 10 records to show in a table, only show the first and last 5\n",
    "pandas.set_option('display.max_rows', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9c29f-cae4-42f9-9b7f-eddd251ab36c",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Introduction to the BH100 Dataset](#Introduction-to-BH100-)\n",
    "* [Wrangling Too Many Cases](#Wrangling-Too-Many-Cases-)\n",
    "    * [Random Selection](#Random-Selection-)\n",
    "    * [Purposeful Selection by Attribute(s)](#Purposeful-Selection-by-Attribute(s)-)\n",
    "    * [Building Your Own Selection Attribute](#Building-Your-Own-Selection-Attribute-)\n",
    "* [Wrangling Too Many Attributes](#Wrangling-Too-Many-Attributes-)\n",
    "    * [Thematic Selection](#Thematic-Selection-)\n",
    "    * [Pattern-Driven Selection](#Pattern-Driven-Selection-)\n",
    "    * [Question-Driven Selection](#Question-Driven-Selection-)\n",
    "* [More About This Dataset](#About-the-Billboard-Hot-100-Dataset-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bfdb6-631b-460a-bdd9-d82ec8680735",
   "metadata": {},
   "source": [
    "# Introduction to BH100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1b602-4394-4423-b7ed-5771a903e29e",
   "metadata": {},
   "source": [
    "Let's check out the dataset. In the setup code above, we read the csv file and used the `pandas` library to process this information and turn it into a data frame called `bh100`. The line below, which just says `bh100`, will give a brief summary of the contents of the dataframe so you can check and make sure it has been read and processed correctly. Since there are too many cases to list, you will see the first five rows and the last five rows of the dataset, with \"...\" in the middle to indicate there are more cases that are not shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efcc6a-b779-43a7-9561-69fb86c4dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the contents of bh100.csv into a dataframe\n",
    "bh100 = pandas.read_csv(\"bh100.csv\")\n",
    "\n",
    "bh100 # show us the contents of the new data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da1cc7c-544a-457a-8453-dc9751b31f33",
   "metadata": {},
   "source": [
    "At the bottom of the output from our code above, we can see the specific dimensions of this dataset. It has 29494 cases (rows), and 35 attributes (columns). Below, we model the different wrangling strategies described in _How to be Choosy_ to reduce the size and/or complexity of the `bh100` dataset so that it is more appropriate for different educational applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0125b-0617-4689-bdd0-0ec688f36329",
   "metadata": {},
   "source": [
    "# Wrangling Too Many Cases <a id=\"cases\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2b639-da2c-421b-bb81-c4efae1b0b48",
   "metadata": {},
   "source": [
    "## Random Selection <a id=\"random\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fb4ff-fb21-41d5-8cc8-6c1261d76a5e",
   "metadata": {},
   "source": [
    "Random selection extracts random rows from a large dataset to create one of a more manageable size. This is the most appropriate strategy for downsizing a dataset while preserving a representative snapshot of the full phenomenon to be studied. Starting with your dataframe, you can create a random selection from that dataframe using the sample() method. Below, we select exactly 5000 random cases from the `bh100` dataset and save them as a new reduced dataset called bh100reduced. If you run the code more than once, you will see that different cases are included in the dataset output each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f4c28-29f2-40d7-bd2e-a4a7475f4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh100random = bh100.sample(5000) # put a randomly selected 5000 rows in bh100reduced\n",
    "bh100random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb755a90-d28a-4e33-9864-0f222acc504a",
   "metadata": {},
   "source": [
    "A related technique is interpolated selection, or selecting every <i>n</i>th row of a data table. This might be useful when the order of the data matters (for example, if records are organized by date and you are interested in modeling patterns over time). However, we do not recommend interpolated selection unless you have a specific reason for using this method, because it can also lead to unintentionally non-random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319f9eb-476e-4098-ac02-72d280606c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh100reduced = bh100.iloc[::6, :] # put every 6th row of bh100 in bh100reduced\n",
    "bh100reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3913c9-7e33-4a80-83a7-191e219b58c9",
   "metadata": {},
   "source": [
    "## Purposeful Selection by Attribute(s) <a id=\"purposeful\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9750466-95e8-4e79-b0b8-8524382a32c6",
   "metadata": {},
   "source": [
    "Purposeful selection involves reducing a dataset so that it only includes records with certain characteristics related to one or more attributes. This method is appropriate if you suspect that the majority of records in your too-large dataset are not useful or usable for your intended activity. To get all the cases in a dataframe that meet certain conditions, use the expression dataframe[condition]. Below, we want only the songs that reached 1 in the charts, represented by cases in bh100 where the value of ‘Highest BH100 Position’ is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9aed3-8d52-4593-9571-e9441af1115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topsongs = bh100['Highest BH100 Position']==1  # for each song, see if the highest position was 1\n",
    "bh100reduced = bh100[topsongs]                 # put only top song records in the reduced dataset\n",
    "bh100reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b85b67-bdac-41c6-84fd-32359cb8efd4",
   "metadata": {},
   "source": [
    "## Building Your Own Selection Attribute <a id=\"byo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2626c0e-4f3f-473a-b4fa-3695d6e46a99",
   "metadata": {},
   "source": [
    "There are other ways of creating a smaller dataset based on information that is not already available in the dataset itself. These could be specific cases that you identify manually, or cases you might identify using computer code to extract some new, meaningful information from the attributes you already have. These techniques allow teachers and students the most customization, but they require careful planning to select and identify which cases should be included and to consider how those decisions will shape what analyses and claims are appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbab21e-0482-47d6-90c9-e08d8f2b4ab6",
   "metadata": {},
   "source": [
    "### By Identifying Specific Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babaa38-f2cc-483e-afb5-51f1f22eb187",
   "metadata": {},
   "source": [
    "Sometimes, you may want to create a small dataset through manually selecting a small number of cases from a larger data corpus. This can be helpful, for example, if you would like students to explore the meaning of different attributes and measures using a small, familiar dataset before diving into larger-scale analyses. One way to do this is by building a list of the ID numbers or _indices_ of the songs you want to include. Below is code you can use to find out what your favorite song indices are and use those to create a list. In the Toxic Release Inventory notebook, we show how you can construct a list directly using text searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344a3a4-3368-4d99-b369-c6a61a139567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the lines below find the indices of the songs you want to include by name\n",
    "# For example, let's see if the dataset has Adele's velvety epic \"Skyfall.\"\n",
    "# If the song with this title is in the dataset, the second line of code will output\n",
    "# the full record for the song. If a song with the title does not exist in the dataset, \n",
    "# it will output an empty table.\n",
    "\n",
    "favoriteSong = bh100['Song Name']==\"Skyfall\" # look up the song by name\n",
    "bh100[favoriteSong]                          # show the full record of the song\n",
    "\n",
    "# The number in bold, in the first column of the table above, is the song's ID\n",
    "# Try looking for a few of your favorite songs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26165a3a-5488-4984-a616-c13fe64e9e5b",
   "metadata": {},
   "source": [
    "Once you have collected the indices of songs you want to include in the dataset, you can use a list of these numbers to create the new subset with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca670eb-c594-4944-a36a-30006eddebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might not immediately know what songs we want to include, and would benefit\n",
    "# from searching the full dataset for certain attributes. Let's check out what was \n",
    "# going on in 1995, a year that may or may not reflect someone's carefree adolescent years.\n",
    "# Notice that here, the year is not in quotes because it is an integer, not a string:\n",
    "\n",
    "carefreeSongs = bh100['Year Released']==1995 # look up the song by year\n",
    "bh100[carefreeSongs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc27948-823a-4b02-b1c8-745da5284af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the list below to store the indices of songs you've identified to include in your subset.\n",
    "# Try finding some of your favorite songs above and adding them to the list!\n",
    "mySongIndices = [10386,29091,9635,21252,5452,11140,4153]\n",
    "\n",
    "#once you have a list of indices for the records you want to keep, you can make your new dataset\n",
    "bh100reduced = pandas.DataFrame([bh100.loc[i] for i in mySongIndices])\n",
    "bh100reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b2bd0-b18d-4e9c-a6a3-c2a02d34fdad",
   "metadata": {},
   "source": [
    "### Using Code to Construct a New Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3bbf9-0636-4a59-80fc-65b8d7d094f2",
   "metadata": {},
   "source": [
    "In other cases, you may want to use more complex queries with code to reduce the number of cases in a dataset to a focused set. Below, we look for songs that have versions of the word \"love\" in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea21c45-c636-446c-94f2-9c140b6ed462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the list below to store the indices of cases to include in your subset\n",
    "# We use \"Love|love|LOVE\" so that the search will look for each capitalization.\n",
    "# The pipe character | means \"or,\" so that songs with any of these words are included.\n",
    "# Try editing the code to find other versions of the word \"love\" as well.\n",
    "# You can also try editing the code to find song titles that have certain\n",
    "# word combinations, like \"love\" and \"you\" using the & character instead of |.\n",
    "searchFor = bh100['Song Name'].str.contains(\"Love|love|LOVE\")\n",
    "bh100reduced = bh100[searchFor]\n",
    "bh100reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60265-c3da-4a75-a5af-42e6f6d941a9",
   "metadata": {},
   "source": [
    "# Wrangling Too Many Attributes <a id=\"attributes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9686ec0-c66b-4bb5-9342-f6dd8ab5b913",
   "metadata": {},
   "source": [
    "Another common issue when using large public datasets for educational purposes is the problem of too many attributes. Environmental datasets such as the Toxic Release Inventory dataset we use in the other notebook example can include hundreds of specific indicators; survey datasets from organizations such as the Pew Research Center similarly report scores of questions per participant. Even this Billboard Hot 100 dataset has too many attributes to comfortably review in a python notebook. While having access to so many attributes can enable the pursuit of a variety of investigative questions, it can easily become overwhelming. Working with these datasets requires planning and thoughtfulness to consider which attributes are actually connected to one's research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10588bb7-ce20-4139-8fa3-8a5aa1b60023",
   "metadata": {},
   "source": [
    "## Thematic Selection <a id=\"thematic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2a9fa-03a6-4e30-b5ba-5c6f81ab7228",
   "metadata": {},
   "source": [
    "Thematic selection involves splitting a dataset up into related, but distinct, groups of attributes that are more likely to be conceptually or statistically related to one another. Thematic attribute selection can be especially useful for jigsaw-like activities, in which different groups explore different aspects of an interconnected system. Below, we create thematic groups by the different types of information we have about each song. You can then access a dataset with a reduced number of selected attributes by calling the dataframe with the attribute group name in brackets. You can include attributes from multiple groups using the plus sign: `dataframe[selection1+selection2]`. Below, we create a reduced dataset with only basic song information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e8431-a848-4509-aaaa-204700cf4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our thematic categories using lists of column names\n",
    "basicInfo = ['Song Name','Performer','Year Released','Month Released','Track Duration','Album']\n",
    "popularity = ['Highest BH100 Position','First Week BH100','Last Week BH100','Weeks on BH100']\n",
    "spotifyInfo = ['Spotify Popularity','Spotify Track Id','Spotify Track Preview URL']\n",
    "genre = ['Spotify Genre Full List','Rock','Latin Pop','Rap','Dance','Novelty','Adult Standards','Rhythm N Blues']\n",
    "performanceFeatures = ['Explicit','Speech-iness','Acoustic-ness','Instrumental-ness','Live-ness']\n",
    "emotionFeatures = ['Danceability','Energy','Valence']\n",
    "soundFeatures = ['Key','Loudness','Mode','Tempo','Time Signature']\n",
    "\n",
    "# use brackets to reference only the columns associated with one category.\n",
    "bh100BasicInfo = bh100[basicInfo]\n",
    "bh100BasicInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f624a1-84c6-4fc1-aaa3-c4abbda5c207",
   "metadata": {},
   "source": [
    "## Pattern-Driven Selection <a id=\"pattern\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94cc38d-7ec9-4dde-ab5e-993479e02495",
   "metadata": {},
   "source": [
    "Similar to thematic selection, pattern-driven selection allows educators and students to focus only on the attributes that are known to align with investigative or pedagogical goals. Whereas thematic selection focuses on the real-world relationships and system components that a student may wish to explore, pattern selection focuses attention on particular mathematical relationships and analytic methods that the dataset makes available for investigation.\n",
    "\n",
    "Below, we demonstrate one example of pattern-driven selection that focuses on continuous measures that describe various acoustic features of songs included in the BH100 dataset. We use a heatmap of pairwise correlations between these attributes to get a quick sense of whether and what types of relationships may exist between these acoustic features. This reveals a range of correlation strengths and directions (ranging from -0.14 to 0.68), suggesting that this specific pattern-driven grouping may provide students opportunities to visualize and practice describing, comparing, and reasoning about correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38263846-34a4-49ab-94ad-ad9f0059b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's limit our dataset to continuous measures that describe\n",
    "# musical features of songs, and see what patterns emerge among \n",
    "# this group.\n",
    "\n",
    "toExplore = ['Danceability',\n",
    "             'Energy',\n",
    "             'Valence',\n",
    "             'Loudness',\n",
    "             'Tempo']\n",
    "\n",
    "viz = bh100[toExplore].corr()\n",
    "\n",
    "sns.heatmap(viz, annot=True)\n",
    "plt.title('Correlation Matrix of Selected Variables')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03c107-b43a-4496-aae5-cf56f8913b19",
   "metadata": {},
   "source": [
    "## Question-Driven Selection <a id=\"question\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777eafd9-4bda-4757-9ba2-ac0c3cbb440f",
   "metadata": {},
   "source": [
    "Question-driven selection involves identifying the attributes within a dataset that are most appropriate for addressing a particular question–whether that question is posed by a student, or posed as a driving question within curriculum materials themselves. We suggest making 3-5 attributes available for a given question in these curricular configurations. When students are constructing their own questions, we recommend encouraging them to also brainstorm what kind of data they would need to address their questions _before_ they have access to a dataset. Once they are provided with access, we suggest asking students to make predictions and construct hypotheses about what patterns they might find. \n",
    "\n",
    "Below, we assemble data to address the simple descriptive question, \"How has the longevity of songs on the charts changed over the years?\" One might ask students to quickly verbally describe or sketch their predictions. Then, after observing that the dataset includes information about the year a song is released and the number of weeks it appeared on the Billboard charts, they might be encouraged to return to their predictions and make them more concrete using the specific measures and ranges they observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b978f8-d23c-47ed-945d-afde31eeacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how has the longevity of songs on the charts changed over the years?\n",
    "\n",
    "toExplore = ['Song Name','Year Released','Weeks on BH100'] #\n",
    "\n",
    "bh100[toExplore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462fbfce-60ea-46f4-9272-a7c589301d0e",
   "metadata": {},
   "source": [
    "Below, we create a scatterplot with each song arranged by year of release on the x-axis and number of weeks on the Billboard Hot 100 chart on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b4b9a-240d-4f7e-902b-1569c5f3fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6)) # Adjust figure size if needed\n",
    "plt.scatter(bh100['Year Released'], bh100['Weeks on BH100'])\n",
    "\n",
    "# Calculate the line of best fit (linear regression)\n",
    "slope, intercept = np.polyfit(bh100['Year Released'], bh100['Weeks on BH100'], 1)\n",
    "\n",
    "# Create the line of best fit equation\n",
    "line = slope * bh100['Year Released'] + intercept\n",
    "\n",
    "plt.plot(bh100['Year Released'], line, color='red', label='Line of Best Fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ee6d4-728c-4c21-bfd6-520f88abf843",
   "metadata": {},
   "source": [
    "Both the scatterplot and the line of best fit suggest there has been some increase in the number of total weeks a song has remained on the Billboard Hot 100 charts. However, it is difficult to tell without more analysis whether this is simply the function of increased variability, or an overall increase in song longevity on the charts. These questions can lead to additional investigations that are conducive to additional modeling methods or hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b2484-59a2-4edc-94af-53bda3c0c98e",
   "metadata": {},
   "source": [
    "# About the Billboard Hot 100 Dataset <a id='about'></a>\n",
    "\n",
    "This dataset includes every song that's ever appeared on the Billboard Hot 100 Charts (August 1958-May 2021). Only some of the available attributes are initially shown. Use the \"attributes\" tab in the \"Choosy\" window to select which attributes or attribute groups to show and hide.\n",
    "\n",
    "### About the Attribute Groups\n",
    "Each record includes Basic Info such as the Song Name, Performer, the Month and Year released; Popularity measures such as the song's highest Billboard position and the number of weeks the song stayed on the the Hot 100 list; and a list of the Genre(s) represented by the song. When available, each song record also includes information scraped from Spotify including the Spotify ID, URL, and popularity on the Spotify app. A number of Spotify-generated measures of musical features are described in Performance Features exploring the song's \"speechiness,\" \"liveness,\" and other inferred performance features; Emotion Features exploring inferred features such as the song's energy level and valence; and Sound Features such as the song's tempo, time signature, and loudness. \n",
    "\n",
    "### About the Attributes\n",
    "- Song Name\n",
    "- Performer\n",
    "- Year Released\n",
    "- *Highest BH100 Position* was computed this from \"Hot Stuff\" database using min position listing for this SongID\n",
    "- Spotify Popularity\n",
    "- *Danceability* describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "- *Energy* is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "- *Loudness* of a track is measured in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
    "- *Speechiness* detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "- *Valence* is a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "- *Tempo* is estimated in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "\n",
    "### History and Purpose\n",
    "This dataset was initially imported into CODAP in Summer 2022 for a teacher workshop assosicated with the Writing Data Stories project (IIS-1900606). It was updated in Spring 2023, and is being used as part of the Writing Data Stories project and the City University of New York's Computing Integrated Teacher Education (CUNY CITE) program. \n",
    "\n",
    "### Data Sources and Data Cleaning\n",
    "This dataset was constructed by Sean Miller (github handle: HipsterVizData) using APIs to download Billboard Hot 100 (BH100) and Spotify (S) data. The full dataset and additional information about its original construction can be accessed at this link. Michelle Wilkerson of the WDS team  merged the two tables in the dataset by mapping the BH100 Song Name attribute to the Spotify SongID, removing all Spotify records that did not have a corresponding BH100 entry but retaining BH100 songs that did not have corresponding Spotify entries. She imported attribute descriptions from the original dataset, editing a few descriptions for readability at the middle school level, consolidated music genres into 7 major genre flags while retaining the full genre list as a separate attribute; and removed several attributes for simplicity. She also grouped Spotify-generated song features into three categories (Performance, Emotion, and Sound Features) visible in the \"Choosy\" menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463152de-3484-4582-819f-6f5d7ae905a2",
   "metadata": {},
   "source": [
    "The development of these materials was supported by the National Science Foundation under Grant No. IIS-1900606"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
